{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "川柳生成器.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/franseal/project_X/blob/master/%E5%B7%9D%E6%9F%B3%E7%94%9F%E6%88%90%E5%99%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "9pu88Ef6_6ay",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#準備"
      ]
    },
    {
      "metadata": {
        "id": "8hIGtWOs_cI3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vqY5xyF3_-ug",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install janome"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Y3mzdmIyAHj5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!apt-get -q -y install swig\n",
        "!apt install aptitude\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n7pU2uZlAaqF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install chainer\n",
        "!pip install cupy-cuda92"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JDcQxXmwAvur",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!pip install tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dUMfqfPV4v3Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import chainer\n",
        "chainer.print_runtime_info()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GnDvjI8ekzpC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "!wget http://www.cl.ecei.tohoku.ac.jp/nlp100/data/neko.txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ijfzjvuBA-CB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#分かち書き"
      ]
    },
    {
      "metadata": {
        "id": "owlwKYhuA5ye",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "import MeCab\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OfnWMlyZBEM6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "mecab = MeCab.Tagger(\"-Ochasen\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iUmgDPUOBLG_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-PgJeMMMBQhD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text = open(\"neko.txt\").readlines()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ard0yK2bBTR6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wakati = []\n",
        "for t in tqdm(text):\n",
        "  for row in mecab.parse(t).split(\"\\n\"):\n",
        "    row = row.split(\"\\t\")\n",
        "    if len(row) > 3:\n",
        "      wakati.append(row[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hfmG-PJ9BfVJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "wakati = np.array(wakati)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a9C4Ac8iWH4M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "word2id = defaultdict(lambda: len(word2id))\n",
        "for w in wakati:\n",
        "  word2id[w]\n",
        "  \n",
        "len(word2id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7kkSmF1tWzoK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "id2word = {i:w for w,i in word2id.items()}\n",
        "len(id2word)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2H6lTwOoW58H",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "_wakati = wakati[1:]\n",
        "_wakati = _wakati[_wakati != \"\\u3000\"]\n",
        "sentences = []\n",
        "st = 0\n",
        "for i,w in enumerate(_wakati):\n",
        "  if w == \"。\":\n",
        "    sentences.append(_wakati[st:i+1])\n",
        "    st = i + 1\n",
        "    \n",
        "print(len(sentences))\n",
        "sentences[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pbCLDL12XO79",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def wlist_to_id(sen):\n",
        "  return [word2id[w] for w in sen]\n",
        "\n",
        "train_data = [wlist_to_id(sen) for sen in sentences]\n",
        "train_data[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UF86YLzqXbJk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#ネットワーク"
      ]
    },
    {
      "metadata": {
        "id": "JRgKRLzwXXkJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import chainer\n",
        "from chainer.backends import cuda\n",
        "from chainer import Function, gradient_check, report, training, utils, Variable\n",
        "from chainer import datasets, iterators, optimizers, serializers\n",
        "from chainer import Link, Chain, ChainList\n",
        "import chainer.functions as F\n",
        "import chainer.links as L\n",
        "from chainer.training import extensions\n",
        "import cupy as cp\n",
        "import random\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xv9m2_VkX3XM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class SentenceGenerator(Chain):\n",
        "  def __init__(self, vocab_size, n_embed=100, n_mid=400):\n",
        "    super().__init__()\n",
        "    with super().init_scope():\n",
        "      self.embed = L.EmbedID(vocab_size, n_embed)\n",
        "      self.lstm = L.NStepLSTM(n_layers=2, in_size=n_embed, out_size=n_mid, dropout=0)\n",
        "      self.out = L.Linear(n_mid, vocab_size)\n",
        "      \n",
        "  def __call__(self, list_of_sentences):\n",
        "    xs = [self.embed(Variable(sen[ :-1])) for sen in list_of_sentences]\n",
        "    ts = [           Variable(sen[1:  ])  for sen in list_of_sentences]\n",
        "    \n",
        "    hs, cs, ys = self.lstm(None, None, xs)\n",
        "    loss = 0.0\n",
        "    for y, t in zip(ys,ts):\n",
        "      z = self.out(y)\n",
        "      loss += F.softmax_cross_entropy(z, t)\n",
        "    return loss\n",
        "  \n",
        "  def predict(self, prefix):\n",
        "    xs = [self.embed(Variable(prefix))]\n",
        "    hs, cs, ys = self.lstm(None, None, xs)\n",
        "    y = ys[0]\n",
        "    z = self.out(y)\n",
        "    return F.softmax(z)[-1].data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SftHocumZrkV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = SentenceGenerator(vocab_size=len(word2id))\n",
        "model.to_gpu(0)\n",
        "\n",
        "optimizer = optimizers.Adam()\n",
        "optimizer.setup(model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R0B0s0_9Z4d1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#学習"
      ]
    },
    {
      "metadata": {
        "id": "8kVrRKBiZ2R5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tqdm import trange\n",
        "\n",
        "batchsize = 128\n",
        "for epoch in range(30): #学習回数\n",
        "  shuffled = np.random.permutation(len(train_data))\n",
        "  sum_loss = 0.0\n",
        "  n=0\n",
        "  \n",
        "  for i in trange(0, len(train_data), batchsize):\n",
        "    ids = shuffled[i:i+batchsize]\n",
        "    xs = [cp.array(train_data[i]) for i in ids]\n",
        "    \n",
        "    model.cleargrads()\n",
        "    loss = model(xs)\n",
        "    loss.backward()\n",
        "    optimizer.update()\n",
        "    \n",
        "    sum_loss += loss.data\n",
        "    #print(loss.data)\n",
        "    n += len(ids)\n",
        "    \n",
        "  print(\"Epoch {} : loss {}\".format(epoch, sum_loss / n))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ucZg0ivUbJ00",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#生成\n",
        "引数はsen, 最初の生成で上位いくつワードを取ってくるか, 以降の生成回数"
      ]
    },
    {
      "metadata": {
        "id": "67rKlY6yaf7V",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from janome.tokenizer import Tokenizer\n",
        "import re\n",
        "target_word1 = \"、\"\n",
        "target_word2 = \"。\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a2ReVkTebNnO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def first(sen, seisei):\n",
        "  keeplist = []\n",
        "  result = []\n",
        "  \n",
        "  for i in range(1):\n",
        "    with chainer.using_config(\"train\", False):\n",
        "      pr = model.predict(cp.array(sen))\n",
        "    pr = chainer.cuda.to_cpu(pr)\n",
        "    pr = np.argsort(pr)[::-1]\n",
        "    \n",
        "    for j in range(seisei):\n",
        "      sen1 = copy.deepcopy(sen)\n",
        "      sen1.append(int(pr[j]))\n",
        "      keeplist.append(sen1)\n",
        "      \n",
        "  t = Tokenizer()\n",
        "  wordcount = 0\n",
        "  word = \"\"\n",
        "  for i in range(len(keeplist)):\n",
        "    s = \"\".join([id2word[i] for i in keeplist[i]])\n",
        "    if target_word1 in s or target_word2 in s:\n",
        "      break\n",
        "    for token in t.tokenize(s, stream=True):\n",
        "      match = re.search(r\"[ャュョヮ]\", token.reading)\n",
        "      wordcount += (len(token.reading))\n",
        "      if match:\n",
        "        wordcount -= (len(re.findall(r\"[ャュョヮ]\", token.reading)))\n",
        "    if wordcount == 5:\n",
        "      word = (token.part_of_speech.split(\",\")[0])\n",
        "      if word == \"助詞\":\n",
        "        result.append(keeplist[i])\n",
        "    wordcount = 0\n",
        "    word = \"\"\n",
        "    \n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "K03ZVnQdccz-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def second(sen, seisei, repeat):\n",
        "  keeplist = []\n",
        "  result = []\n",
        "  \n",
        "  for i in range(1):\n",
        "    with chainer.using_config(\"train\", False):\n",
        "      pr = model.predict(cp.array(sen))\n",
        "    pr = chainer.cuda.to_cpu(pr)\n",
        "    pr = np.argsort(pr)[::-1]\n",
        "    \n",
        "    for j in range(seisei):\n",
        "      sen1 = copy.deepcopy(sen)\n",
        "      sen1.append(int(pr[j]))\n",
        "      keeplist.append(sen1)\n",
        "      \n",
        "  for i in range(repeat):\n",
        "    with chainer.using_config(\"train\", False):\n",
        "      sen = copy.deepcopy(keeplist[i])\n",
        "      pr = model.predict(cp.array(sen))\n",
        "    pr = chainer.cuda.to_cpu(pr)\n",
        "    pr = np.argsort(pr)[::-1]\n",
        "    \n",
        "    for j in range(seisei):\n",
        "      sen1 = copy.deepcopy(sen)\n",
        "      sen1.append(int(pr[j]))\n",
        "      keeplist.append(sen1)\n",
        "      \n",
        "  t = Tokenizer()\n",
        "  wordcount = 0\n",
        "  word = \"\"\n",
        "  for i in range(len(keeplist)):\n",
        "    s = \"\".join([id2word[i] for i in keeplist[i]])\n",
        "    if target_word1 in s or target_word2 in s:\n",
        "      break\n",
        "    for token in t.tokenize(s, stream=True):\n",
        "      match = re.search(r\"[ャュョヮ]\", token.reading)\n",
        "      wordcount += (len(token.reading))\n",
        "      if match:\n",
        "        wordcount -= (len(re.findall(r\"[ャュョヮ]\", token.reading)))\n",
        "    if wordcount == 12:\n",
        "      word = (token.part_of_speech.split(\",\")[0])\n",
        "      if word == \"助詞\":\n",
        "        result.append(keeplist[i])\n",
        "    wordcount = 0\n",
        "    word = \"\"\n",
        "    \n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kc2wxAJBekDW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def third(sen, seisei, repeat):\n",
        "  keeplist = []\n",
        "  result = []\n",
        "  \n",
        "  for i in range(1):\n",
        "    with chainer.using_config(\"train\", False):\n",
        "      pr = model.predict(cp.array(sen))\n",
        "    pr = chainer.cuda.to_cpu(pr)\n",
        "    pr = np.argsort(pr)[::-1]\n",
        "    \n",
        "    for j in range(seisei):\n",
        "      sen1 = copy.deepcopy(sen)\n",
        "      sen1.append(int(pr[j]))\n",
        "      keeplist.append(sen1)\n",
        "      \n",
        "  for i in range(repeat):\n",
        "    with chainer.using_config(\"train\", False):\n",
        "      sen = copy.deepcopy(keeplist[i])\n",
        "      pr = model.predict(cp.array(sen))\n",
        "    pr = chainer.cuda.to_cpu(pr)\n",
        "    pr = np.argsort(pr)[::-1]\n",
        "    \n",
        "    for j in range(seisei):\n",
        "      sen1 = copy.deepcopy(sen)\n",
        "      sen1.append(int(pr[j]))\n",
        "      keeplist.append(sen1)\n",
        "      \n",
        "  t = Tokenizer()\n",
        "  wordcount = 0\n",
        "  word = \"\"\n",
        "  for i in range(len(keeplist)):\n",
        "    s = \"\".join([id2word[i] for i in keeplist[i]])\n",
        "    if target_word1 in s or target_word2 in s:\n",
        "      break\n",
        "    for token in t.tokenize(s, stream=True):\n",
        "      match = re.search(r\"[ャュョヮ]\", token.reading)\n",
        "      wordcount += (len(token.reading))\n",
        "      if match:\n",
        "        wordcount -= (len(re.findall(r\"[ャュョヮ]\", token.reading)))\n",
        "    if wordcount == 17:\n",
        "      word = (token.part_of_speech.split(\",\")[0])\n",
        "      #if word == \"助詞\":\n",
        "      result.append(keeplist[i])\n",
        "    wordcount = 0\n",
        "    word = \"\"\n",
        "    \n",
        "  return result"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3-yCMg9EiO6h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "f_rank = 80 #上五の上位数\n",
        "s_rank = 10 #中七の上位数\n",
        "s_create = 20 #中七の生成数\n",
        "t_rank = 10 #下五の上位数\n",
        "t_create = 20 #下五の生成数\n",
        "\n",
        "sen = [word2id[\"男\"]] #渡す単語"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-mRzerKHh-NJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "kari = []\n",
        "res2 = []\n",
        "res3 = []\n",
        "\n",
        "res = first(sen, f_rank) #上位\n",
        "print(res)\n",
        "\n",
        "for i in range(len(res)):\n",
        "  sen = res[i]\n",
        "  kari = second(sen, s_rank, s_create)\n",
        "  for j in range(len(kari)):\n",
        "    res2.append(kari[j])\n",
        "print(res2)\n",
        "\n",
        "for i in range(len(res2)):\n",
        "  sen = res2[i]\n",
        "  kari = third(sen, t_rank, t_create)\n",
        "  for j in range(len(kari)):\n",
        "    res3.append(kari[j])\n",
        "print(res3)\n",
        "\n",
        "for i in range(len(res3)):\n",
        "  s = \"\".join([id2word[i] for i in res3[i]])\n",
        "  print(s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aIPdXdbKDuBz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}